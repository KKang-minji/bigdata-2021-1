---
title: "Mid-term coding test"
author: "jiho yeo"
date: "4/12/2021"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Import libraries

깔려있지 않은 패키지는 `install.packages("package_name")`을 통해 설치하세요. 
미리 아래의 library를 모두 실행시킨 후 문제를 푸시면 됩니다.

```{r cars}
library(tidyverse)
library(tidytext)
library(janeaustenr)
library(stringr)
library(gutenbergr)
```

---

## 1. 감성분석

Jane Austen 책들(Sense & Sensibility, Pride & Prejudice, Mansfield Park, Emma, Northanger Abbey, Persuasion)의 챕터진행에 따른 Sentiment score의 변화를 시각화 하시오. 

(Hint: `austen_books()` 데이터를 사용. Sentiment score는 bing 감성어사전 기반으로 챕터별 positive 단어수 - negative 단어수로 계산. `ggplot2` 의 `geom_col`을 통해서 x축을 Chapter로 y축을 sentiment_score로 하여 책별로 시각화) (15점)


```{r sentiment}
get_sentiments("bing")

tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%               #group으로 묶인 데이터 그룹 해제
  unnest_tokens(sentiment_score, text)

tidy_books

bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
                     #bing이라는 감성사전을 사용해서 join
  count(sentiment_score, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts


bing_word_counts %>%
  group_by(sentiment) %>%    
  top_n(10) %>%              #n이 10이라는 것(10개만 추출)
  ungroup() %>%
  mutate(word = reorder(sentiment_score, n)) %>%   #word를 n에 따라 정렬
  ggplot(aes(chapter, sentiment_score, fill = sentiment)) +
            
  geom_col(show.legend = FALSE) +        
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)


```

---

## 2. tf-idf

tf-idf (term frequency-inverse document frequency)는 문서 내에 상대적으로 중요한 단어를 알려준다. 아래에 나열된 책들을 가지고 tf-idf score를 계산하고 책별로 높은 tf-idf를 가지는 단어 15개를 시각화 하시오 (15점).  

---

2-1. 아래의 `books` 데이터를 tidy text 형태로 변환 후, title별 word의 사용빈도수(n)를 계산하시오 (5점). 

```{r}
titles <- c("Twenty Thousand Leagues under the Sea", 
            "The War of the Worlds",
            "Pride and Prejudice", 
            "Great Expectations")

books <- gutenberg_works(title %in% titles) %>%
  gutenberg_download(meta_fields = "title",
                     mirror ="http://mirrors.xmission.com/gutenberg/") %>%
  unnest_tokens(gutenberg_id, text) %>%
  count(title, word, sort = TRUE)

books



#책별 전체사용하는 단어수
total_words <- title %>% 
  group_by(book) %>% 
  summarize(total = sum(n))

total_words

#book_words에 total_words를 left_join시킴
book_words <- left_join(book_words, total_words, by="book")
book_words <- book_words %>% left_join(total_words, by="book")
#책별, 많이 사용하는 단어별로  
book_words %>% arrange(book,desc(n)) %>% View()
```

```{r}



```

2-2. 위에서 계산한 title별 word의 사용빈도수 데이터를 기반으로, 책별로 높은 tf-idf를 가지는 단어 10개씩을 추출하여 시각화 하시오 (Hint: `geom_col`을 사용하여 시각화. `slice_max`를 이용하여 title별 tf-idf가 높은 10개의 단어를 추출) (10점).

```{r}



```

---

## 3. 특정 단어와 함께 사용된 단어 파악

---

3-1. `austen_books()` 데이터를 사용해서 "Sense & Sensibility" 책의 텍스트를 2-grams로 tokenization 하고, 각각을 word1, word2 컬럼으로 구분하시오 (Hint: `separate` 명령어를 사용) (5점).

```{r}
austen_bigrams <- austen_books() %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
austen_bigrams
austen_bigrams %>%
  count(bigrams, sort = TRUE)
bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

head(bigrams_separated)


```

3-2. 위에서 생성한 데이터를 기반으로 `stop_words`에 있는 불용어를 데이터셋에서 제거한 후, 'miss'로 시작하는 단어와 가장 많이 사용된 단어 10개를 추출하시오 (Hint: `filter`, `count`, `slice_max` 함수를 이용) (5점).

```{r}
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%   #!=NOT
  filter(!word2 %in% stop_words$word)
         #Word1(2)가 stop_words$word에 포함안된 것만 뽑겠다.

bigrams_filtered %>%
  filter(word2 == "miss") %>%
  count(book, word1, sort = TRUE)

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

head(bigram_counts)
#drop_na:NA제거
bigram_counts %>% drop_na



```

