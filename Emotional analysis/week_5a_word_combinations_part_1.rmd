---
title: "word combinations"
author: "jiho yeo"
date: "3/26/2021"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Relationships between words: n-grams and correlations {#ngrams}

So far we've considered words as individual units, and considered their relationships to sentiments or to documents. However, many interesting text analyses are based on the relationships between words, whether examining which words tend to follow others immediately, or that tend to co-occur within the same documents.

In this chapter, we'll explore some of the methods tidytext offers for calculating and visualizing relationships between words in your text dataset. This includes the `token = "ngrams"` argument, which tokenizes by pairs of adjacent words rather than by individual ones. We'll also introduce two new packages: [ggraph](https://github.com/thomasp85/ggraph), which extends ggplot2 to construct network plots, and [widyr](https://github.com/dgrtwo/widyr), which calculates pairwise correlations and distances within a tidy data frame. Together these expand our toolbox for exploring text within the tidy data framework.

## Tokenizing by n-gram

We've been using the `unnest_tokens` function to tokenize by word, or sometimes by sentence, which is useful for the kinds of sentiment and frequency analyses we've been doing so far. But we can also use the function to tokenize into consecutive sequences of words, called **n-grams**. By seeing how often word X is followed by word Y, we can then build a model of the relationships between them.

We do this by adding the `token = "ngrams"` option to `unnest_tokens()`, and setting `n` to the number of words we wish to capture in each n-gram. When we set `n` to 2, we are examining pairs of two consecutive words, often called "bigrams":


여러개 단어 묶어서
```{r austen_bigrams}
library(dplyr)
library(tidytext)
library(janeaustenr)

?unnest_tokens

austen_bigrams <- austen_books() %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
                   #ngrams:여러개 단어 묶어서 실행, n: 단어수
austen_bigrams
```

This data structure is still a variation of the tidy text format. It is structured as one-token-per-row (with extra metadata, such as `book`, still preserved), but each token now represents a bigram.

```{block, type = "rmdnote"}
Notice that these bigrams overlap: "sense and" is one token, while "and sensibility" is another.
```

### Counting and filtering n-grams

Our usual tidy tools apply equally well to n-gram analysis. We can examine the most common bigrams using dplyr's `count()`:

```{r, dependson = "austen_bigrams"}
austen_bigrams %>%
  count(bigram, sort = TRUE)
```

As one might expect, a lot of the most common bigrams are pairs of common (uninteresting) words, such as `of the` and `to be`: what we call "stop-words" (see Chapter \@ref(tidytext)). This is a useful time to use tidyr's `separate()`, which splits a column into multiple based on a delimiter. This lets us separate it into two columns, "word1" and "word2", at which point we can remove cases where either is a stop-word.

```{r bigram_counts, dependson = "austen_bigrams"}
library(tidyr)

austen_bigrams 

#separate: 한컬럼에 있는 단어를 두 컬럼으로 나눔
bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

head(bigrams_separated)

#필요없는 단어들 제거
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%   #!=NOT
  filter(!word2 %in% stop_words$word)
         #Word1(2)가 stop_words$word에 포함안된 것만 뽑겠다.

# new bigram counts: 
#filtered로 얼마나 이단어가 많이 쓰였는지 정렬
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

#drop_na:NA제거
bigram_counts %>% drop_na

#결과: 사람을 지칭하는 단어들이 많이 사용 되었다.
```
es (whether first and last or with a salutation) are the most common pairs in Jane Austen books.

In other analyses, we may want to work with the recombined words. tidyr's `unite()` function is the inverse of `separate()`, and lets us recombine the columns into one. Thus, "separate/filter/count/unite" let us find the most common bigrams not containing stop-words.


`unite()`: 다시 합칠때
```{r bigrams_united, dependson = "bigram_counts"}
bigrams_united <- bigrams_filtered %>%
  drop_na %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united    #필터링되어서 합쳐짐
```

In other analyses you may be interested in the most common trigrams, which are consecutive sequences of 3 words. We can find this by setting `n = 3`:

`trigrams()`: 단어 세개를 나눔
```{r}
austen_books() %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE)
```

### Analyzing bigrams
### bigrams 분석
특정한 단어앞에 붙은 형용사, 명사가 뭔지 궁금할때 사용

This one-bigram-per-row format is helpful for exploratory analyses of the text. As a simple example, we might be interested in the most common "streets" mentioned in each book:

어떤 길이 이 소설에서 많이 등장했는지 알아보기
```{r bigrams_filtered_street, dependson = "bigram_counts"}
bigrams_filtered %>%
  filter(word2 == "street") %>%
  count(book, word1, sort = TRUE)
```

A bigram can also be treated as a term in a document in the same way that we treated individual words. For example, we can look at the tf-idf (Chapter \@ref(tfidf)) of bigrams across Austen novels. These tf-idf values can be visualized within each book, just as we did for words (Figure \@ref(fig:bigramtfidf)).

책별로 어떤 bigram단위로 어떤 말이 많이 쓰였는지 알아보기
```{r bigram_tf_idf, dependson = "bigram_counts"}
bigrams_united

bigram_tf_idf <- bigrams_united %>%
  count(book, bigram) %>%
  bind_tf_idf(term = bigram,
              document = book,
              n = n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf

#결론: ex)Mansfield Park 책에서 sir thomas단어가 다른책에서 보다 상대적으로 많이 쓰였다는 것을 알 수 있음.
```
책별로 어떤 bigram단위로 어떤 말이 많이 쓰였는지 시각화
```{r bigramtfidf, dependson = "bigram_tf_idf", echo = FALSE, fig.width=6, fig.height=8, fig.cap = "Bigrams with the highest tf-idf from each Jane Austen novel"}
library(ggplot2)

bigram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  group_by(book) %>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>%
  mutate(bigram = reorder(bigram, tf_idf)) %>%
  ggplot(aes(tf_idf, bigram, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ book, ncol = 2, scales = "free") +
  labs(x = "tf-idf of bigram", y = NULL)
```

Much as we discovered in Chapter \@ref(tfidf), the units that distinguish each Austen book are almost exclusively names. We also notice some pairings of a common verb and a name, such as "replied elizabeth" in Pride & Prejudice, or "cried emma" in Emma.

There are advantages and disadvantages to examining the tf-idf of bigrams rather than individual words. Pairs of consecutive words might capture structure that isn't present when one is just counting single words, and may provide context that makes tokens more understandable (for example, "pulteney street", in Northanger Abbey, is more informative than "pulteney"). 

However, the per-bigram counts are also *sparser*: a typical two-word pair is rarer than either of its component words. Thus, bigrams can be especially useful when you have a very large text dataset.

