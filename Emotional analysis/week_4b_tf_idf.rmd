---
title: "tf-idf"
author: "jiho yeo"
date: "3/25/2021"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 참고자료

- tf-idf에 대한 한글 설명: https://wikidocs.net/31698
- tf-idf는 문서 내에 상대적으로 중요한 단어를 알려준다
- tf는 Term Frequency의 약자로 문서에서 해당 단어가 몇개인지를 나타냄
- idf는 Inverse Document Frequency의 약자로 DF(Document Frequency)의 역수
- tf-idf = tf * idf :빈도수(특정단어가 특정문서내에 얼마나 중요한가를 나타냄)



# Analyzing word and document frequency: tf-idf {#tfidf}
#단어의 빈도, 모든문서에서 이단어가 얼마나 등장하는지의 역수

How to quantify what a document is about. Can we do this by looking at the words that make up the document? One measure of how important a word may be is its *term frequency* (tf), how frequently a word occurs in a document, as we examined in Chapter \@ref(tidytext). There are words in a document, however, that occur many times but may not be important; in English, these are probably words like "the", "is", "of", and so forth. 

We might take the approach of adding words like these to a list of stop words and removing them before analysis, but it is possible that some of these words might be more important in some documents than others. A list of stop words is not a very sophisticated approach to adjusting term frequency for commonly used words.

Another approach is to look at a term's *inverse document frequency* (idf), which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents. This can be combined with term frequency to calculate a term's *tf-idf* (the two quantities multiplied together), the frequency of a term adjusted for how rarely it is used. 
문서에 대한 내용을 수량화하는 방법. 우리가 문서를 구성하는 단어들을 보고 이것을 할 수 있을까요? 단어가 얼마나 중요한지 보여주는 한 가지 척도는 \@ref(tidytext)에서 살펴본 바와 같이 문서에서 단어가 얼마나 자주 발생하는지를 나타내는 *항 빈도*(tf)이다. 그러나 문서에는 여러 번 발생하지만 중요하지 않을 수도 있는 단어들이 있다; 영어에서, 이것들은 아마도 "the", "is", "of" 등과 같은 단어일 것이다.

우리는 이러한 단어를 정지 단어 목록에 추가하고 분석 전에 제거하는 방법을 택할 수도 있지만, 이러한 단어 중 일부는 다른 단어보다 일부 문서에서 더 중요할 수 있다. 정지 단어 목록은 일반적으로 사용되는 단어에 대한 용어 빈도를 조정하는 매우 정교한 접근법이 아니다.

또 다른 접근법은 용어의 *역문서 빈도*(idf)를 살펴보는 것인데, 일반적으로 사용되는 단어의 가중치를 줄이고 문서 집합에서 많이 사용되지 않는 단어의 가중치를 증가시킨다. 이 값을 항 주파수와 결합하여 항의 *tf-idf*(두 개의 양을 함께 곱한 값)를 계산할 수 있으며, 항 사용 빈도에 대해 조정된 항 주파수를 계산할 수 있습니다.

#stop_word보다 tf-idf를 사용해서 상대적인 사용빈도를 살펴보자

```{block, type = "rmdnote"}
The statistic **tf-idf** is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites.
**tf-idf** 통계는 단어가 문서 모음(또는 말뭉치)의 문서에 얼마나 중요한지 측정하기 위한 것이다. 예를 들어 소설 모음의 소설 또는 웹 사이트 모음의 웹 사이트 하나에 얼마나 중요한지를 측정하기 위한 것이다.
```

It is a rule-of-thumb or heuristic quantity; while it has proved useful in text mining, search engines, etc., its theoretical foundations are considered less than firm by information theory experts. The inverse document frequency for any given term is defined as
그것은 경험적 법칙 또는 경험적 양이다; 텍스트 마이닝, 검색 엔진 등에서 유용하다고 증명되었지만, 그것의 이론적 토대는 정보 이론 전문가들에 의해 확고하지 못한 것으로 간주된다. 주어진 항에 대한 역문서 빈도는 다음과 같이 정의된다.

$$idf(\text{term}) = \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}$$
전체문서의 수/ 특정단어를 담고있는 문서의 수

##시험##
tf_idf 만드는 계산볍 (왜 곱하는지 알아야함)
```{r}
exam <- data.frame(word=c("cat","the"), n=c(10,1000), total = 10000)

exam$tf <- exam$n/exam$total
exam

# 전체 10권의 책 중에 cat이라는 단어를 쓴 책은 3권, 
# the라는 단어를 쓴 책은10권 중 10권 모두라고 가정
exam$idf <- c(log(10/3), log(10/10))
exam

exam$tf_idf <- exam$tf * exam$idf
exam
```


We can use tidy data principles, as described in Chapter \@ref(tidytext), to approach tf-idf analysis and use consistent, effective tools to quantify how important various terms are in a document that is part of a collection.

## Term frequency in Jane Austen's novels

Let's start by looking at the published novels of Jane Austen and examine first term frequency, then tf-idf. We can start just by using dplyr verbs such as `group_by()` and `join()`. What are the most commonly used words in Jane Austen's novels? (Let's also calculate the total words in each novel here, for later use.)

```{r book_words}
library(dplyr)
library(janeaustenr)
library(tidytext)

#책별, 단어별 빈도
book_words <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE)

book_words

#책별 전체사용하는 단어수
total_words <- book_words %>% 
  group_by(book) %>% 
  summarize(total = sum(n))

total_words

#book_words에 total_words를 left_join시킴
book_words <- left_join(book_words, total_words, by="book")
book_words <- book_words %>% left_join(total_words, by="book")
#책별, 많이 사용하는 단어별로  
book_words %>% arrange(book,desc(n)) %>% View()
```

There is one row in this `book_words` data frame for each word-book combination; `n` is the number of times that word is used in that book and `total` is the total words in that book.

The usual suspects are here with the highest `n`, "the", "and", "to", and so forth. In Figure \@ref(fig:plottf), let's look at the distribution of `n/total` for each novel, the number of times a word appears in a novel divided by the total number of terms (words) in that novel. This is exactly what term frequency is.

히스토그램
```{r plottf, dependson = "book_words", fig.height=6, fig.width=6, fig.cap="Term frequency distribution in Jane Austen's novels"}
library(ggplot2)

book_words %>% mutate(tf=n/total)
#tf의 분포 시각화
ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, ncol = 2, scales = "free_y")
```
결과: 많이 쓰는 단어만 쓰고 적게 쓰는 단어들은 거의 안쓴다고 봄
많은 빈도로 쓰고 있는 단어들은 중요하지 않은 단어일 가능성 높음.
적은 값이 중요함.
(매우 긴 꼬리를 가짐)

There are very long tails to the right for these novels (those extremely rare words!) that we have not shown in these plots. These plots exhibit similar distributions for all the novels, with many words that occur rarely and fewer words that occur frequently.

## The `bind_tf_idf()` function

The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents

In this case, the group of Jane Austen's novels as a whole. Calculating tf-idf attempts to find the words that are important (i.e., common) in a text, but not *too* common. Let's do that now.

The `bind_tf_idf()` function in the tidytext package takes a tidy text dataset as input with one row per token (term), per document. One column (`word` here) contains the terms/tokens, one column contains the documents (`book` in this case), and the last necessary column contains the counts, how many times each document contains each term (`n` in this example). We calculated a `total` for each book for our explorations in previous sections, but it is not necessary for the `bind_tf_idf()` function; the table only needs to contain all the words in each document.


term:단어마다 쪼갬
단어마다의 tf, idf
```{r tf_idf, dependson = "book_words"}
?bind_tf_idf

book_tf_idf <- book_words %>%
  bind_tf_idf(term = word, document = book, n = n)

book_words

book_tf_idf
```

Notice that idf and thus tf-idf are zero for these extremely common words. These are all words that appear in all six of Jane Austen's novels, so the idf term (which will then be the natural log of 1) is zero. The inverse document frequency (and thus tf-idf) is very low (near zero) for words that occur in many of the documents in a collection; this is how this approach decreases the weight for common words. The inverse document frequency will be a higher number for words that occur in fewer of the documents in the collection. 

Let's look at terms with high tf-idf in Jane Austen's works.

상대적인 단어의 중요성(책별로 많이쓴 단어 보여줌)
```{r desc_idf, dependson = "tf_idf"}
book_tf_idf %>%
  select(-total) %>%
  arrange(desc(tf_idf)) #내림차순
```

Here we see all proper nouns, names that are in fact important in these novels. None of them occur in all of novels, and they are important, characteristic words for each text within the corpus of Jane Austen's novels. 

```{block, type = "rmdnote"}
Some of the values for idf are the same for different terms because there are 6 documents in this corpus and we are seeing the numerical value for $\ln(6/1)$, $\ln(6/2)$, etc. 
```

Let's look at a visualization for these high tf-idf words in Figure \@ref(fig:plotseparate).

```{r plotseparate, dependson = "plot_austen", fig.height=8, fig.width=6, fig.cap="Highest tf-idf words in each of Jane Austen's Novels"}
library(forcats)

book_tf_idf_vis <- book_tf_idf %>%
  group_by(book) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup()

book_tf_idf_vis

ggplot(book_tf_idf_vis, aes(tf_idf, fct_reorder(word, tf_idf),
                #fct_reorder: 재정렬되어 높은 값부터 보여줌
                            fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)


?fct_reorder
```

## A corpus of physics texts

Let's work with another corpus of documents, to see what terms are important in a different set of works. In fact, let's leave the world of fiction and narrative entirely. Let's download some classic physics texts from Project Gutenberg and see what terms are important in these works, as measured by tf-idf. 

Let's download [*Discourse on Floating Bodies* by Galileo Galilei](http://www.gutenberg.org/ebooks/37729), [*Treatise on Light* by Christiaan Huygens](http://www.gutenberg.org/ebooks/14725), [*Experiments with Alternate Currents of High Potential and High Frequency* by Nikola Tesla](http://www.gutenberg.org/ebooks/13476), and [*Relativity: The Special and General Theory* by Albert Einstein](http://www.gutenberg.org/ebooks/5001).

This is a pretty diverse bunch. They may all be physics classics, but they were written across a 300-year timespan, and some of them were first written in other languages and then translated to English. Perfectly homogeneous these are not, but that doesn't stop this from being an interesting exercise!


책불러오기
```{r eval = FALSE}
library(gutenbergr)
physics <- gutenberg_download(c(37729, 14725, 13476, 30155), 
                              meta_fields = "author")
```

Now that we have the texts, let's use `unnest_tokens()` and `count()` to find out how many times each word was used in each text.


작가별, 단어별 사용
```{r physics_words, dependson = "physics"}
physics_words <- physics %>%
  unnest_tokens(word, text) %>%
  count(author, word, sort = TRUE)

physics_words
```

Here we see just the raw counts; we need to remember that these documents are all different lengths. Let's go ahead and calculate tf-idf, then visualize the high tf-idf words in Figure \@ref(fig:physicsseparate).

```{r physicsseparate, dependson = "plot_physics", fig.height=6, fig.width=6, fig.cap="Highest tf-idf words in each physics texts"}
plot_physics <- physics_words %>%
  bind_tf_idf(word, author, n) %>%
  mutate(author = factor(author, levels = c("Galilei, Galileo",
                                            "Huygens, Christiaan", 
                                            "Tesla, Nikola",
                                            "Einstein, Albert")))
#factor로 바꾸면 숫자로 받아들임. 메모리용량때문에 바꿈
plot_physics


plot_physics %>% 
  group_by(author) %>% 
  slice_max(tf_idf, n = 15) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word, fill = author)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~author, ncol = 2, scales = "free")
```

Very interesting indeed. One thing we see here is "_k_" in the Einstein text?!


"_k_"이런 텍스트만 찾아줌
```{r dependson = "physics"}
library(stringr)

physics %>% 
  filter(str_detect(text, "_k_")) %>% 
  select(text)
```

Let's remove some of these less meaningful words to make a better, more meaningful plot. Notice that we make a custom list of stop words and use `anti_join()` to remove them; this is a flexible approach that can be used in many situations. We will need to go back a few steps since we are removing words from the tidy data frame.

빼고싶은 text 제거후 시각화
```{r mystopwords, dependson = "plot_physics", fig.height=6, fig.width=6, fig.cap="Highest tf-idf words in classic physics texts"}

mystopwords <- tibble(word = c("eq", "co", "rc", "ac", "ak", "bn", 
                                   "fig", "file", "cg", "cb", "cm",
                               "ab", "_k", "_k_", "_x", "ad"))

#mystopwords에 있는 단어를 physics_words에서 찾아서 없애라
physics_words <- physics_words %>% anti_join(mystopwords, by = "word")

physics_words 

plot_physics <- physics_words %>%
  bind_tf_idf(word, author, n) %>%
  mutate(word = str_remove_all(word, "_")) %>%
  group_by(author) %>% 
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  mutate(word = reorder_within(word, tf_idf, author)) %>%
  mutate(author = factor(author, levels = c("Galilei, Galileo",
                                            "Huygens, Christiaan",
                                            "Tesla, Nikola",
                                            "Einstein, Albert")))

ggplot(plot_physics, aes(word, tf_idf, fill = author)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~author, ncol = 2, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```
